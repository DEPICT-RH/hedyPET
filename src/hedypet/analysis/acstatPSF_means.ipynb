{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da236a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hedypet.utils import DERIVATIVES_ROOT, load_splits, get_time_frames_midpoint\n",
    "from nifti_dynamic.utils import load_tac\n",
    "from nifti_dynamic.patlak import roi_patlak\n",
    "from parse import parse\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import pandas as pd \n",
    "import os "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "229bdaf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "subs = load_splits()[\"all\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2ddda7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def load_json(file_path):\n",
    "    with open(file_path,\"r\") as handle:\n",
    "        d = json.load(handle)\n",
    "    return d\n",
    "\n",
    "region_names = {\n",
    "    'ts_total' :load_json(\"/homes/hinge/Projects/hedyPET/src/hedypet/analysis/seg-total-classes.json\"),\n",
    "    'synthseg' :load_json(\"/homes/hinge/Projects/hedyPET/src/hedypet/analysis/seg-synthseg_classes.json\"),\n",
    "    'synthsegparc' : load_json(\"/homes/hinge/Projects/hedyPET/src/hedypet/analysis/seg-synthseg_classes.json\"),\n",
    "    'ts_tissue' :load_json(\"/homes/hinge/Projects/hedyPET/src/hedypet/analysis/seg-tissue-classes.json\"),\n",
    "    'ts_body' : {\"1\":\"trunk\",\"2\":\"extremeties\"},\n",
    "    'totalimage' : {\"1\":\"body\"},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d952bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]/tmp/ipykernel_2858264/558898987.py:16: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  vals = {\"mu\":float(mu_organ),\"std\":float(std_organ),\"n\":int(n_organ)}\n",
      "100%|██████████| 100/100 [03:37<00:00,  2.18s/it]\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(df_path := \"acstatPSF_means.pkl\"):\n",
    "    data = []\n",
    "\n",
    "    for sub in tqdm(subs):\n",
    "        tacs_root = (DERIVATIVES_ROOT / f\"tacs/{sub}/acstatPSF\")\n",
    "        tacs = list(tacs_root.glob(\"**/tac*\"))\n",
    "\n",
    "        #IF tacs not relevant for mean analysis\n",
    "        tacs_if = [x for x in tacs if \"aorta\" in str(x)]\n",
    "        tacs_organs = [x for x in tacs if x not in tacs_if]\n",
    "                \n",
    "        # Try all combinations of inputs functions and num_frames\n",
    "        for tac_organ_path in tacs_organs:\n",
    "            mu_organ, std_organ, n_organ = load_tac(tac_organ_path)\n",
    "            tags = parse('{}/tacs/{sub}/acstatPSF/{task}/erosion-{erosion}/tac_{ix}',str(tac_organ_path)).named\n",
    "            vals = {\"mu\":float(mu_organ),\"std\":float(std_organ),\"n\":int(n_organ)}\n",
    "            vals.update(tags)\n",
    "            vals[\"region\"] = region_names[vals[\"task\"]][vals[\"ix\"]]\n",
    "            data.append(vals)\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_pickle(df_path)\n",
    "else:\n",
    "    df = pd.read_pickle(df_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0829709c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "columns: Index(['mu', 'std', 'n', 'sub', 'task', 'erosion', 'ix', 'region'], dtype='object')\n",
      "Unique task: ['ts_total' 'synthseg' 'synthsegparc' 'ts_tissue' 'ts_body' 'totalimage']\n",
      "Unique erosion: ['0' '1']\n",
      "Available normalizations: ['suv', 'sul_janma', 'sul_james', 'sul_decazes', 'sul_auto', 'luv', 'sul_auto_decazes']\n",
      "Rows: 49750\n",
      "Subjects: 100\n"
     ]
    }
   ],
   "source": [
    "from hedypet.utils import get_norm_consts\n",
    "\n",
    "print(\"columns:\", df.columns)\n",
    "print(\"Unique task:\", df.task.unique())\n",
    "print(\"Unique erosion:\", df.erosion.unique())\n",
    "print(\"Available normalizations:\", list(get_norm_consts(\"sub-000\").keys()))\n",
    "print(\"Rows:\",len(df))\n",
    "print(\"Subjects:\", df[\"sub\"].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "92dd551c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by erosion and task, calculate mean and std of mu\n",
    "from hedypet.utils import get_norm_consts\n",
    "\n",
    "def get_normalizer(normalization_method):\n",
    "    norm_dict = {sub:get_norm_consts(sub) for sub in subs}\n",
    "    for k in norm_dict:\n",
    "        norm_dict[k][\"none\"] = 1000\n",
    "    return lambda x: 1000*x[\"mu\"]/norm_dict[x[\"sub\"]][normalization_method]\n",
    "\n",
    "\n",
    "def filter_and_combine_regions(df,regions):\n",
    "    group_cols = [\"sub\",\"erosion\",\"task\"]\n",
    "    results = []\n",
    "\n",
    "    for k, v in regions.items():\n",
    "        mask = df.region.isin(v) if isinstance(v, list) else df.region == v\n",
    "        filtered = df[mask]\n",
    "        \n",
    "        if isinstance(v, list):\n",
    "            result = filtered.groupby(group_cols).apply(\n",
    "                lambda x: pd.Series({\n",
    "                    'mu': (x.mu * x.n).sum() / x.n.sum(),\n",
    "                    'n': x.n.sum()\n",
    "                })\n",
    "            ).reset_index()\n",
    "        else:\n",
    "            result = filtered\n",
    "        \n",
    "        result['region'] = k\n",
    "        results.append(result)\n",
    "\n",
    "    return pd.concat(results, ignore_index=True)\n",
    "\n",
    "def summarize_activity_and_volume(df,normalization=\"suv\",ml_per_vox=1.65 * 1.65 * 2.0 / 1000):\n",
    "\n",
    "    df[\"volume\"] = df.n *  ml_per_vox\n",
    "    df[normalization] = df.apply(get_normalizer(normalization),axis=1)\n",
    "    summary_df = df.groupby(['erosion', 'region',\"task\"]).agg({\n",
    "        normalization: ['mean', 'std', 'count'],\n",
    "        'volume': ['mean','std']\n",
    "    })\n",
    "\n",
    "    # Flatten column names\n",
    "    summary_df.columns = [f'{normalization}_mean', f'{normalization}_std', f'{normalization}_count', 'volume_mean', 'volume_std']\n",
    "    summary_df = summary_df.reset_index()\n",
    "\n",
    "    # Add standard error of the mean for both SUV and volume\n",
    "    summary_df[f'{normalization}_sem'] = (summary_df[f'{normalization}_std'] / (summary_df[f'{normalization}_count'] ** 0.5)).round(4)\n",
    "    summary_df['volume_sem'] = (summary_df['volume_std'] / (summary_df[f'{normalization}_count'] ** 0.5)).round(4)\n",
    "    return summary_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27df492d",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'PosixPath' object has no attribute 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[169]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mdf_ki\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtac_if_path\u001b[49m\u001b[43m.\u001b[49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstr\u001b[49m[\u001b[32m0\u001b[39m]\n",
      "\u001b[31mAttributeError\u001b[39m: 'PosixPath' object has no attribute 'str'"
     ]
    }
   ],
   "source": [
    "df_ki.tac_if_path.iloc[0].aook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "0aef1a4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(542421)"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ki.tac_if_path.apply(lambda x:\"aortasegments/erosion-0/tac_1\" in str(x)).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83f69c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(49311)"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ki = pd.read_pickle(\"patlak_ki.pkl\")\n",
    "().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "530a4d57",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2858264/28093563.py:29: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  result['region'] = k\n",
      "/tmp/ipykernel_2858264/28093563.py:20: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  result = filtered.groupby(group_cols).apply(\n",
      "/tmp/ipykernel_2858264/28093563.py:29: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  result['region'] = k\n",
      "/tmp/ipykernel_2858264/28093563.py:20: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  result = filtered.groupby(group_cols).apply(\n",
      "/tmp/ipykernel_2858264/28093563.py:29: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  result['region'] = k\n",
      "/tmp/ipykernel_2858264/28093563.py:20: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  result = filtered.groupby(group_cols).apply(\n",
      "/tmp/ipykernel_2858264/28093563.py:20: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  result = filtered.groupby(group_cols).apply(\n",
      "/tmp/ipykernel_2858264/28093563.py:29: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  result['region'] = k\n",
      "/tmp/ipykernel_2858264/28093563.py:20: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  result = filtered.groupby(group_cols).apply(\n",
      "/tmp/ipykernel_2858264/28093563.py:29: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  result['region'] = k\n",
      "/tmp/ipykernel_2858264/28093563.py:20: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  result = filtered.groupby(group_cols).apply(\n",
      "/tmp/ipykernel_2858264/28093563.py:29: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  result['region'] = k\n",
      "/tmp/ipykernel_2858264/28093563.py:20: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  result = filtered.groupby(group_cols).apply(\n",
      "/tmp/ipykernel_2858264/28093563.py:20: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  result = filtered.groupby(group_cols).apply(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "regions = {\n",
    "    \"spleen\":\"spleen\",\n",
    "    \"kidneys\": [\"kidney_right\",\"kidney_left\"],\n",
    "    \"liver\":\"liver\",\n",
    "    \"stomach\":\"stomach\",\n",
    "    \"lungs\": ['lung_upper_lobe_left', 'lung_lower_lobe_left','lung_upper_lobe_right', 'lung_middle_lobe_right','lung_lower_lobe_right'],\n",
    "    'colon':\"colon\",\n",
    "    'subcutaneous_fat':\"subcutaneous_fat\",\n",
    "    \"muscle\":\"skeletal_muscle\",\n",
    "    'visceral_fat':\"visceral_fat\",\n",
    "    \"gray_matter\":['Left-Cerebral-Cortex', 'Right-Cerebral-Cortex'], \n",
    "    \"white_matter\": ['Right-Cerebral-White-Matter' , 'Left-Cerebral-White-Matter'],\n",
    "}\n",
    "\n",
    "def create_latex_table(data_dfs, normalizations):\n",
    "    \"\"\"\n",
    "    Creates table with structure:\n",
    "    Region | Erosion | Volume | SUV | SUL\n",
    "    liver  | eros1   | xx±yy  | xx±yy | xx±yy\n",
    "           | eros2   | xx±yy  | xx±yy | xx±yy\n",
    "    \"\"\"\n",
    "    # Get latex versions of all dataframes\n",
    "    latex_dfs = {}\n",
    "    for norm in normalizations:\n",
    "        df = data_dfs[norm].copy()\n",
    "        df[f'{norm}_latex'] = '$' + df[f'{norm}_mean'].round(2).astype(str) + r' \\pm ' + df[f'{norm}_sem'].round(3).astype(str) + '$'\n",
    "        df['volume_latex'] = '$' + df['volume_mean'].round(0).astype(str) + r' \\pm ' + df['volume_sem'].round(0).astype(str) + '$'\n",
    "        latex_dfs[norm] = df[['erosion', 'region', f'{norm}_latex', 'volume_latex']]\n",
    "    \n",
    "    # Merge all dataframes\n",
    "    combined = latex_dfs[normalizations[0]]\n",
    "    for norm in normalizations[1:]:\n",
    "        combined = combined.merge(latex_dfs[norm], on=['erosion', 'region'], suffixes=('', f'_{norm}'))\n",
    "    \n",
    "    # FIX ROW ISSUE: Remove duplicates if multiple tasks exist\n",
    "    combined = combined.drop_duplicates(subset=['erosion', 'region'])\n",
    "    \n",
    "    # Create LaTeX table with multirow\n",
    "    latex_str = r'\\begin{tabular}{|l|l|' + 'c|' * len(normalizations + ['volume']) + '}\\n'\n",
    "    latex_str += r'\\hline' + '\\n'\n",
    "    \n",
    "    # Header\n",
    "    cols = ['Region', 'Erosion', 'Volume'] + [norm.upper() for norm in normalizations]\n",
    "    header = ' & '.join(cols) + r' \\\\' + '\\n'\n",
    "    latex_str += header\n",
    "    latex_str += r'\\hline' + '\\n'\n",
    "    \n",
    "    # Data rows with multirow for regions\n",
    "    for region in combined.region.unique():\n",
    "        region_data = combined[combined.region == region].sort_values('erosion')\n",
    "        n_erosions = len(region_data)\n",
    "        \n",
    "        for i, (_, row_data) in enumerate(region_data.iterrows()):\n",
    "            if i == 0:\n",
    "                # First row: multirow region name\n",
    "                region_cell = f\"\\\\multirow{{{n_erosions}}}{{*}}{{{region.replace('_', ' ')}}}\"\n",
    "            else:\n",
    "                # Subsequent rows: empty region cell\n",
    "                region_cell = \"\"\n",
    "            \n",
    "            # Build data row\n",
    "            data_row = region_cell\n",
    "            data_row += f\" & {row_data['erosion']}\"\n",
    "            data_row += f\" & {row_data['volume_latex']}\"\n",
    "            for norm in normalizations:\n",
    "                data_row += f\" & {row_data[f'{norm}_latex']}\"\n",
    "            data_row += r' \\\\' + '\\n'\n",
    "            \n",
    "            latex_str += data_row\n",
    "        \n",
    "        # Add horizontal line after each region group\n",
    "        latex_str += r'\\hline' + '\\n'\n",
    "    \n",
    "    latex_str += r'\\end{tabular}' + '\\n'\n",
    "    latex_str = latex_str.replace(\"_\",\" \")\n",
    "    \n",
    "    with open('/homes/hinge/Projects/hedyPET/manuscript/tables/table_data.tex', 'w') as f:\n",
    "        f.write(latex_str)\n",
    "    \n",
    "    # Also return a regular DataFrame for inspection\n",
    "    table_data = []\n",
    "    for region in combined.region.unique():\n",
    "        region_data = combined[combined.region == region].sort_values('erosion')\n",
    "        for _, row_data in region_data.iterrows():\n",
    "            row = [region.replace('_', ' '), row_data['erosion'], row_data['volume_latex']]\n",
    "            for norm in normalizations:\n",
    "                row.append(row_data[f'{norm}_latex'])\n",
    "            table_data.append(row)\n",
    "    \n",
    "    result_df = pd.DataFrame(table_data, columns=cols)\n",
    "    return result_df\n",
    "\n",
    "\n",
    "df_ki = pd.read_pickle(\"patlak_ki.pkl\")\n",
    "df_ki = df_ki[(df_ki.frames==5) & (df_ki.tac_if_path.apply(lambda x: \"aortasegments/erosion-1/tac_4\" in str(x)))]\n",
    "df_ki = df_ki.drop(columns=[\"tac_if_path\",\"tac_organ_path\",\"frames\",\"if_tag\"])\n",
    "df_ki = df_ki.rename(columns={\"slope\":\"mu\"})\n",
    "df_ki.mu*=1000\n",
    "\n",
    "# Usage\n",
    "df_comb = filter_and_combine_regions(df, regions)\n",
    "df_ki = filter_and_combine_regions(df_ki,regions)\n",
    "\n",
    "df_suv = summarize_activity_and_volume(df_comb, \"suv\")  \n",
    "df_sul = summarize_activity_and_volume(df_comb, \"sul_decazes\")\n",
    "df_ki = summarize_activity_and_volume(df_ki, \"none\")\n",
    "\n",
    "data_dfs = {'suv': df_suv, 'sul_decazes': df_sul,\"none\":df_ki}\n",
    "normalizations = ['suv', 'sul_decazes',\"none\"]\n",
    "\n",
    "table = create_latex_table(data_dfs, normalizations)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hedyPET",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
